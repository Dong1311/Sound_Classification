{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":142598,"sourceType":"datasetVersion","datasetId":3151},{"sourceId":928025,"sourceType":"datasetVersion","datasetId":500970}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-04T16:30:15.425045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport random\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models import resnet18, convnext_tiny, efficientnet_v2_s,shufflenet_v2_x2_0,mobilenet_v3_small,mobilenet_v3_large\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\ninput_path = '/kaggle/input/'\n\nurbansound8k_path = input_path + 'urbansound8k/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"urban_df = pd.read_csv(urbansound8k_path + \"UrbanSound8K.csv\")\nurban_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"urban_df['relative_path'] = '/fold' + urban_df['fold'].astype(str) + '/' + urban_df['slice_file_name'].astype(str)\nurban_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom torchaudio import transforms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AudioUtil():\n    \n    def open_audio(audio_path):\n        signal, sample_rate = torchaudio.load(audio_path)\n        return (signal, sample_rate)\n    \n    def rechannel(aud, new_channel):\n        sig, sr = aud\n\n        if (sig.shape[0] == new_channel):\n          # Nothing to do\n          return aud\n\n        if (new_channel == 1):\n          # Convert from stereo to mono by selecting only the first channel\n          resig = sig[:1, :]\n        else:\n          # Convert from mono to stereo by duplicating the first channel\n          resig = torch.cat([sig, sig])\n\n        return ((resig, sr))\n    \n    def resample(aud, new_sample_rate):\n        \n        signal, sample_rate = aud\n        n_channels = signal.shape[0]\n        if (sample_rate != new_sample_rate):\n            resig = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(signal[:1,:])\n            if (n_channels > 1):\n                retwo = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(signal[1:,:])\n                resig = torch.cat([resig,retwo])\n            return ((resig, new_sample_rate))\n        else:\n            return aud\n    def pad_trunc(aud, max_ms):\n        sig, sr = aud\n        num_rows, sig_len = sig.shape\n        max_len = sr // 1000 * max_ms\n\n        if sig_len > max_len:\n            # Truncate the signal to the given length\n            sig = sig[:, :max_len]\n        elif sig_len < max_len:\n            # Calculate the length of padding to add at the beginning and end of the signal\n            pad_begin_len = random.randint(0, max_len - sig_len)\n            pad_end_len = max_len - sig_len - pad_begin_len\n\n            # Pad with 0s\n            pad_begin = torch.zeros((num_rows, pad_begin_len))\n            pad_end = torch.zeros((num_rows, pad_end_len))\n\n            sig = torch.cat((pad_begin, sig, pad_end), 1)\n\n        return (sig, sr)\n    def time_shift(aud, shift_limit):\n        sig,sr = aud\n        _, sig_len = sig.shape\n        shift_amt = int(random.random() * shift_limit * sig_len)\n        return (sig.roll(shift_amt), sr)\n    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n        sig,sr = aud\n        top_db = 80\n\n        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n\n        # Convert to decibels\n        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n        return (spec)\n    def LFCC(aud, sr = 16000, n_lfcc = 64, n_fft=1024, hop_len=None):\n        sig,sr = aud\n        top_db = 80\n        spec = transforms.LFCC(sr, speckwargs={\"n_fft\": n_fft, \"hop_length\": hop_len, \"center\": False}, n_lfcc = n_lfcc)(sig)\n\n        # Convert to decibels\n        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n        return (spec)\n    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n        _, n_mels, n_steps = spec.shape\n        mask_value = spec.mean()\n        aug_spec = spec\n\n        freq_mask_param = max_mask_pct * n_mels\n        for _ in range(n_freq_masks):\n            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n\n        time_mask_param = max_mask_pct * n_steps\n        for _ in range(n_time_masks):\n            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n\n        return aug_spec\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset, random_split\nclass SoundDS(Dataset):\n    def __init__(self, df, data_path):\n        self.df = df\n        self.data_path = str(data_path)\n        self.duration = 4000\n        self.sr = 44100\n        self.channel = 2\n        self.shift_pct = 0.4\n            \n   # ----------------------------\n   # Number of items in dataset\n   # ----------------------------\n    def __len__(self):\n        return len(self.df)    \n      \n   # ----------------------------\n   # Get i'th item in dataset\n   # ----------------------------\n    def __getitem__(self, idx):\n        # Absolute file path of the audio file - concatenate the audio directory with\n        # the relative path\n        audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n        # Get the Class ID\n        class_id = self.df.loc[idx, 'classID']\n\n        aud = AudioUtil.open_audio(audio_file)\n        # Some sounds have a higher sample rate, or fewer channels compared to the\n        # majority. So make all sounds have the same number of channels and same \n        # sample rate. Unless the sample rate is the same, the pad_trunc will still\n        # result in arrays of different lengths, even though the sound duration is\n        # the same.\n        reaud = AudioUtil.resample(aud, self.sr)\n        rechan = AudioUtil.rechannel(reaud, self.channel)\n\n        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n\n        return aug_sgram, class_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myds = SoundDS(urban_df, urbansound8k_path)\n\n# Random split of 80:20 between training and validation\nnum_items = len(myds)\nnum_train = round(num_items * 0.8)\nnum_val = num_items - num_train\ntrain_ds, val_ds = random_split(myds, [num_train, num_val])\n\n# Create training and validation data loaders\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\nval_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class modModel(nn.Module):\n    def __init__(self, num_classes):\n        super(modModel,self).__init__()\n        self.num_classes = num_classes\n        self.conv1x1 = nn.Conv2d(2,3, kernel_size = 1)\n        self.baseModel = mobilenet_v3_large(num_classes = self.num_classes)\n    def forward(self, x):\n        x = self.conv1x1(x)\n        x = self.baseModel(x)\n#         x = F.softmax(x, dim = 1)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = torch.rand((1,2,400,400))\nprint(test.shape)\nmodel = modModel(10)\nx = model(test)\nprint(x.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nchild_counter = 0\nfor child in model.children():\n    print(\" child\", child_counter, \"is:\")\n    print(child)\n    child_counter += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training(model, train_dl, num_epochs):\n    # Loss Function, Optimizer and Scheduler\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n                                                steps_per_epoch=int(len(train_dl)),\n                                                epochs=num_epochs,\n                                                anneal_strategy='linear')\n\n  # Repeat for each epoch\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct_prediction = 0\n        total_prediction = 0\n\n        # Repeat for each batch in the training set\n        for i, data in enumerate(train_dl):\n            # Get the input features and target labels, and put them on the GPU\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            # Normalize the inputs\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            # Keep stats for Loss and Accuracy\n            running_loss += loss.item()\n\n            # Get the predicted class with the highest score\n            _, prediction = torch.max(outputs,1)\n            # Count of predictions that matched the target label\n            correct_prediction += (prediction == labels).sum().item()\n            total_prediction += prediction.shape[0]\n\n            #if i % 10 == 0:    # print every 10 mini-batches\n            #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n    \n        # Print stats at the end of the epoch\n        num_batches = len(train_dl)\n        avg_loss = running_loss / num_batches\n        avg_acc = correct_prediction/total_prediction\n        print(f'Epoch: {epoch}, Loss: {avg_loss}, Accuracy: {avg_acc}')\n\n    # Save model\n    torch.save(model.state_dict(), 'model.pt')\n\n    print('Finished Training')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myModel = modModel(10)\ntraining(myModel, train_dl, 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(model, test_dl):\n    correct_prediction = 0\n    total_prediction = 0\n\n    # Disable gradient updates\n    with torch.no_grad():\n        for data in test_dl:\n            # Get the input features and target labels, and put them on the GPU\n            inputs, labels = data[0].to(device), data[1].to(device)\n\n            # Normalize the inputs\n            inputs_m, inputs_s = inputs.mean(), inputs.std()\n            inputs = (inputs - inputs_m) / inputs_s\n\n            # Get predictions\n            outputs = model(inputs)\n\n            # Get the predicted class with the highest score\n            _, prediction = torch.max(outputs,1)\n            # Count of predictions that matched the target label\n            correct_prediction += (prediction == labels).sum().item()\n            total_prediction += prediction.shape[0]\n    acc = correct_prediction/total_prediction\n    print(f'Accuracy: {acc}, Total items: {total_prediction}')\n    \nmodel_inf = modModel(10)\nmodel_inf.load_state_dict(torch.load('model.pt'))\nmodel_inf = model_inf.to(device)\nmodel_inf.eval()\n\ninference(model_inf, val_dl)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}